Hybrid Stable Reconstruction
AI-Driven Video Frame Reordering & Motion Stabilization
ğŸš€ Overview

Hybrid Stable Reconstruction is a lightweight, CPU-safe algorithm that reconstructs a jumbled or unordered video into a temporally consistent, forward-moving sequence.
By combining semantic scene understanding with optical-flow-based trajectory smoothing, it intelligently restores natural motion in real-world clips â€” such as a person walking smoothly along a path â€” even when the input frames are randomly shuffled.

This hybrid design delivers AI-level intelligence without GPU dependency, making it practical for laptops with integrated graphics (e.g., Intel Iris Xe).

ğŸ¯ Key Features

âœ… Hybrid AI Ordering â€“ Fuses semantic similarity (SegFormer b0) with optical-flow cues for reliable temporal reasoning.
âœ… OFIR-Style Trajectory Smoothing â€“ Applies Savitzkyâ€“Golay filtering to eliminate back-and-forth jumps.
âœ… Lightweight and Fast â€“ Runs efficiently on CPU-only environments.
âœ… Automatic Fallback â€“ Switches to DeepLabV3 when Hugging Face models are unavailable.
âœ… Stable Forward Motion â€“ Produces a natural, directionally consistent output video with minimal jitter.

âš™ï¸ Working Principle

Frame Extraction â€“ Reads all frames from the scrambled input video.

Semantic Fingerprinting â€“ Each frame passes through a pre-trained SegFormer model to generate a compact 64-bin histogram representing its semantic layout.

Similarity Graph Formation â€“ A pairwise similarity matrix encodes visual relatedness between frames.

Hybrid Ordering Algorithm â€“ Greedily selects the next frame that maximizes semantic continuity while respecting motion consistency.

Trajectory Refinement â€“ Optical-flow magnitudes are accumulated and smoothed to ensure monotonic forward progression.

Video Reconstruction â€“ Frames are re-assembled in the refined order, yielding a coherent, forward-moving clip.

ğŸ§© Installation
pip install opencv-python torch torchvision tqdm transformers scipy

â–¶ï¸ Usage

Single-line command:

python hybrid_stable_reconstruct_v2.py --input jumbled_video.mp4 --output reconstructed_stable.mp4 --fps 60


Output files:

reconstructed_stable.mp4 â€“ Final stabilized video

reconstructed_stable.order.txt â€“ Recovered frame order indices

ğŸ“Š Example Outcome

From a completely shuffled walking-sequence video,
Hybrid Stable Reconstruction v2 recovers a visually smooth forward motion with suppressed jitter and no large reversals.

ğŸ§± Technical Highlights
Module	Function
SegFormer (b0)	Extracts high-level semantic scene representations
Optical Flow (Farneback)	Captures local pixel-wise motion direction
Savitzkyâ€“Golay Filter	Smooths cumulative trajectory to enforce forward monotonicity
Hybrid Greedy Ordering	Merges appearance and motion cues for temporal reconstruction
ğŸ’¡ Design Philosophy

The project demonstrates that a smart combination of pre-trained semantic models and classic optical-flow analysis can reconstruct temporal order without any training data or GPU.
This fusion of modern AI perception and traditional vision dynamics offers a practical path toward temporal understanding in video restoration.

âš ï¸ Limitations

Minor micro-jitters may persist in scenes with strong background motion.

Ambiguous, near-identical frames can occasionally swap locally.

Pose-based enhancement could further improve temporal precision (future work).

ğŸ§‘â€ğŸ’» Authorship & Acknowledgement

Developed by: Upasana Bhaumik
Project: Hybrid Stable Reconstruction v2 â€” Semantic + Optical Flow Video Reordering
Â© 2025 Upasana Bhaumik â€” All rights reserved